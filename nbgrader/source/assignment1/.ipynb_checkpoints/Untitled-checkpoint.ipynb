{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9fc7db9f36897a2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 1 \n",
    "### Neural Networks and Fuzzy Logic (2nd Semester 2019-2020)\n",
    "\n",
    "### Instructions:\n",
    "This assignment is designed to check your understanding of linear algebra, assuming you have basic knowlegde of Linear Algebra, this assignment will introudce you to some high level tasks in Linear Algebra, and what's the fun part about it? You'll be implemnting those concepts and visualizing them!! So Let's start.\n",
    "\n",
    "#### Key Concepts: Matrix Inverse, Linear Transforms, Eigendecomposition and all the interesting stuff Linear Algebra has :)\n",
    "\n",
    "##### Please make sure you adhere to the policy of originality of your work, the assignment should reflect your understanding and should be your original, cases of plagiarism will be strictly dealt with as per institute norms. The penalties can include debarrment from the course and registration for the next semester.\n",
    "\n",
    "# Matrices as linear transformations\n",
    "\n",
    "Matrices can be viewed as linear transformations you 'apply' to vectors. Some matrices will rotate your space, others will rescale it etc. So when we apply a matrix to a vector, we end up with a transformed version of the vector. When we say that we 'apply' the matrix to the vector it means that we calculate the dot product of the matrix with the vector. We will start with a basic example of this kind of transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-19c0e98f3e95828d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#No other imports are allowed !!!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b9af1226de0620a9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#utility function, please don't alter this\n",
    "def plot_vectors(vectors, cols, alpha=1):\n",
    "    \"\"\"\n",
    "    Plot set of vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vectors : array-like\n",
    "        Coordinates of the vectors to plot. Each vectors is in an array. For\n",
    "        instance: [[1, 3], [2, 2]] can be used to plot 2 vectors.\n",
    "    cols : array-like\n",
    "        Colors of the vectors. For instance: ['green', 'red'] will display the\n",
    "        first vector in green and the second in red.\n",
    "    alpha : float\n",
    "        Opacity of vectors\n",
    "    Returns:\n",
    "    fig : instance of matplotlib.figure.Figure\n",
    "        The figure of the vectors\n",
    "    \"\"\"\n",
    "    plt.axvline(x=0, color='#A9A9A9', zorder=0)\n",
    "    plt.axhline(y=0, color='#A9A9A9', zorder=0)\n",
    "\n",
    "    for i in range(len(vectors)):\n",
    "        if (isinstance(alpha, list)):\n",
    "            alpha_i = alpha[i]\n",
    "        else:\n",
    "            alpha_i = alpha\n",
    "        x = np.concatenate([[0,0],vectors[i]])\n",
    "        plt.quiver([x[0]],\n",
    "                   [x[1]],\n",
    "                   [x[2]],\n",
    "                   [x[3]],\n",
    "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
    "                  alpha=alpha_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce702182a2c41123",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 3], [2, 6]])\n",
    "print(A)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f315bf803194fcb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "v = np.array([[5], [3]])\n",
    "print(v)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3984d8d6d889d8c6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plot_vectors([v.flatten()],['red'])\n",
    "plt.ylim(0, 6)\n",
    "plt.xlim(0, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58de14d42535ed42",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Inline Question**: did you see that v is flattened before plotting? why do we need to do that? Check out the code for plot_vectors and try to answer this? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-89c617592276dccd",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c70805160037e04",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_linear_transform(v,A):\n",
    "    '''\n",
    "    Apply the linear transformation on v by a matrix A\n",
    "    return the transformed matrix (Av)\n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f7d4f29ea739491",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#DO NOT ALTER THIS CELL\n",
    "#HIDDEN TEST CASES FOR apply_linear_transform\n",
    "### BEGIN HIDDEN TESTS\n",
    "from nose.tools import assert_equal\n",
    "mat0 = np.random.randn(10)\n",
    "mat1 = np.random.randn(10,10)\n",
    "mat2 = np.random.randn(10,10,10)\n",
    "\n",
    "vect1 = np.random.randn(10,10,10)\n",
    "vect2 = np.random.randn(10,10)\n",
    "vect3 = np.random.rand(10)\n",
    "\n",
    "assert np.array_equal(apply_linear_transform(mat0,vect3), mat0.dot(vect3))\n",
    "assert np.array_equal(apply_linear_transform(mat1,vect2), mat1.dot(vect2))\n",
    "assert np.array_equal(apply_linear_transform(mat2,vect1), mat2.dot(vect1))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3caec98ae0f9b128",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let us visualize the transformation on vector v by a matrix A as mentioned above\n",
    "transformed_vector = apply_linear_transform(v,A)\n",
    "plot_vectors([v.flatten(), transformed_vector.flatten()],cols=['red','green'])\n",
    "plt.ylim(0, 6)\n",
    "plt.xlim(0, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-90ab2f47fb9ba388",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "We have seen an example of a vector transformed by a matrix. Now imagine that the transformation of the initial vector gives us a new vector that has the exact same direction. The scale can be different but the direction is the same. Applying the matrix didn't change the direction of the vector. This special vector is called an eigenvector of the matrix. We will see that finding the eigenvectors of a matrix can be very useful.\n",
    "\n",
    "<span class='pquote'>\n",
    "    Imagine that the transformation of the initial vector by the matrix gives a new vector with the exact same direction. This vector is called an eigenvector of ${A}$.\n",
    "</span>\n",
    "\n",
    "This means that ${v}$ is a eigenvector of ${A}$ if ${v}$ and ${Av}$ are in the same direction or to rephrase it if the vectors ${Av}$ and ${v}$ are parallel. The output vector is just a scaled version of the input vector. This scalling factor is $\\lambda$ which is called the **eigenvalue** of ${A}$.\n",
    "\n",
    "$$\n",
    "{Av} = \\lambda{v}\n",
    "$$\n",
    "\n",
    "Talking in terms of eigen values and eigen vector the eigen decomposition of a matrix looks like the following:\n",
    "\n",
    "$$\n",
    "{A}= {V}\\cdot{diag}({\\lambda})\\cdot{V}^{-1}\n",
    "$$\n",
    "\n",
    "We will also verify this property of eigendecomposition.\n",
    "\n",
    "Eigen decomposition is a crucial step for PCA (introduced later)\n",
    "\n",
    "Let's begin!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-947cfea731ce6537",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_eigen_decomposition(matrix):\n",
    "    '''\n",
    "    use numpy to find the eigen decomposition of the matrix\n",
    "    returns eigenValues and the eigenVectors\n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9e1377220b4336dd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#DO NOT ALTER THIS CELL\n",
    "#HIDDEN TEST CASES FOR find_eigen_decomposition\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "matrix = np.random.randn(10,10)\n",
    "matrix1 = np.random.randn(100,10)\n",
    "matrix2 = np.random.randn(20,10)\n",
    "eigenValues,eigenVectors = np.linalg.eig(matrix)\n",
    "eigenValues1,eigenVectors1 = np.linalg.eig(matrix1)\n",
    "eigenValues2,eigenVectors2 = np.linalg.eig(matrix2)\n",
    "\n",
    "from_func_eig,from_func_eig11 = find_eigen_decomposition(matrix)\n",
    "from_func_eig1,from_func_eig111 = find_eigen_decomposition(matrix1)\n",
    "from_func_eig2,from_func_eig222 = find_eigen_decomposition(matrix2)\n",
    "\n",
    "assert np.array_equal(eigenValues,from_func_eig)\n",
    "assert np.array_equal(eigenValues1,from_func_eig1)\n",
    "assert np.array_equal(from_func_eig2,eigenValues2)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b7f2c7683aef8586",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def verify_eigen_decomposition(matrix,eigen_values,eigen_vectors):\n",
    "    \n",
    "    '''\n",
    "    verifies the eigen decomposition property of matrix\n",
    "    return the recontsructed matrix using the property of eigen vectors and eigenvalues\n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ca924fb2cbe495d7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#DO NOT ALTER THIS CELL\n",
    "#HIDDEN TEST CASES FOR verify_eigen_decomposition\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "matrix = np.random.randn(10,10)\n",
    "eigenValues,eigenVectors = np.linalg.eig(matrix)\n",
    "assert np.array_equal(verify_eigen_decomposition(matrix,eigenValues,eigenVectors),matrix)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd8a48317889d004",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "Handling Dimensions is very important in Machine Leanring. The dimensions are all the features of the dataset. For instance, if you are looking at a dataset containing pieces of music, dimensions could be the genre, the length of the piece, the number of instruments, the presence of a singer etc. You can imagine all these dimensions as different columns. When there is only two dimensions, it is very convenient to plot: you can use the $x$ - and  $y$ -axis. Add color and you can represent a third dimension. It is similar if you have tens or hundereds of dimensions, it will just be harder to visualize it.\n",
    "\n",
    "When you have that many dimensions it happens that some of them are correlated. For instance, we can reasonably think that the genre dimension will correlate with the instruments dimensions in our previous example. One way to reduce dimensionality is simply to keep only some of them. The problem is that you loose good information. It would be nice to have a way to reduce these dimensions while keeping all the information present in the data set.\n",
    "\n",
    "The aim of principal components analysis (PCA) is generaly to reduce the number of dimensions of a dataset where dimensions are not completly decorelated. PCA provides us with a new set of dimensions, the principal components (PC).\n",
    "\n",
    "Step-wise PCA is:\n",
    "\n",
    "1) Center the data <br>\n",
    "2) Find the covariance matrix <br>\n",
    "3) Find the Eigen Decompostion <br>\n",
    "4) Choose the principal components <br>\n",
    "5) Find the new dataset Matrix <br>\n",
    "\n",
    "Following [article](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c) also explains PCA very well.\n",
    "Let's implement PCA to get an essence of how dimensions can be reduced, you need to implement the function one by one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50d014c7204eab60",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x = 5*np.random.rand(100)\n",
    "y = 2*x + 1 + np.random.randn(100)\n",
    "\n",
    "#the two dimensions of our dataset\n",
    "x = x.reshape(100, 1)\n",
    "y = y.reshape(100, 1)\n",
    "\n",
    "#The dataset matrix\n",
    "X = np.hstack([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a4b70ccb4531995",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:,0],X[:,1],'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4aace02eb82ebd5c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def center(X):\n",
    "    '''\n",
    "    function to center the data around the mean\n",
    "    modify the return value to return the modified matrix\n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9e373b3dc8185259",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#DO NOT ALTER THIS CELL\n",
    "#HIDDEN TEST CASES FOR center\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "mat1 = np.random.randn(10,10)\n",
    "mat2 = np.random.randn(100,100)\n",
    "\n",
    "mat11 = mat1 - np.mean(mat1,axis=0)\n",
    "mat22 = mat2 - np.mean(mat2,axis=0)\n",
    "\n",
    "assert np.array_equal(mat11,center(mat1))\n",
    "assert np.array_eqaul(mat22,center(mat2))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f2ba4844ba63791f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_covariance_matrix(X):\n",
    "    '''\n",
    "    Find the covariance matrix of X\n",
    "    Return the final matrix\n",
    "    '''\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-23455edbef9a7b84",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#DO NOT ALTER THIS CELL\n",
    "#HIDDEN TEST CASES FOR find_covariance_matrix\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "mat1 = np.random.randn(10,10)\n",
    "mat2 = np.random.randn(100,100)\n",
    "\n",
    "assert np.array_equal(np.cov(mat1),find_covariance_matrix(mat1))\n",
    "assert np.array_equal(np.cov(mat2),find_covariance_matrix(mat2))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dabaa22556056316",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now let's plot the centered data and the eigen vectors in the same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86b9c744415dd515",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use the simulated X, the one we generated in one of the above cells\n",
    "X_centered = center(X)\n",
    "covariance = find_covariance_matrix(X_centered)\n",
    "eigenVals, eigenVecs = find_eigen_decomposition(covariance)\n",
    "\n",
    "\n",
    "plot_vectors(eigenVecs.T, ['red', 'blue'])\n",
    "plt.plot(X_centered[:,0], X_centered[:,1], '*')\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-365c5fe2dd253c56",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def two_dimensional_pca(X_centered, eigenValues, eigenVecs):\n",
    "    '''\n",
    "    using X_centered, eigenValues, eigenVecs as obtained above\n",
    "    apply pca on X_centered and chose the principal component\n",
    "    return the new matrix\n",
    "    '''\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f8a28e82ea7c8e1d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#DO NOT ALTER THIS CELL\n",
    "#HIDDEN TEST CASES FOR two_dimensional_pca\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "X_new = two_dimensional_pca(X_centered, eigenVals, eigenVecs)\n",
    "assert np.array_equal(X_new,X)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bae36247dbb3680b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Inline Questions:** <br> \n",
    "1) Can you identify the color of the line which points to the direction of maximum variance? <br>\n",
    "2) Look at the Graph below and explain if PCA can be applied on this dataset or not? Do give reason. <br>\n",
    "3) Can you apply PCA to categorical variables? (Answer to this will give you hints for the next excercise)\n",
    "\n",
    "![Plot](images/pca_inline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67cd484a33c21216",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on a large dataset\n",
    "## About Dataset:\n",
    "Task: Reducing the dimesions for predicting house prices.\n",
    "The dataset is stored in the file **'data_pca.csv'**, it has 15 numerical columns and 2 categorical columns (columns are labeled as $X0$....$X16$), in an imaginary world, all these columns have certain significance towards prediciton of final house prices. Your task is simple, apply PCA to reduce the number of dimensions. \n",
    "\n",
    "Now implement the following functions for getting the final dataset with reduced dimensions. Keep the top $k (k=0...10)$ principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b2744694d03e082f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_data(name):\n",
    "    '''\n",
    "    read the CSV file and return the corresponding dataframe\n",
    "    don't forget to remove the index column (check the name properly)\n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7c4fe4ab88e97f99",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#input the path top the CSV file\n",
    "#DO NOT ALTER THIS CELL\n",
    "#HIDDEN TEST CASES FOR read_data\n",
    "\n",
    "PATH_TO_CSV_FILE = './data_pca.csv'\n",
    "### BEGIN HIDDEN TESTS\n",
    "df = pd.read_csv(PATH_TO_CSV_FILE).drop(['Unnamed: 0'],axis=1)\n",
    "df2 = read_data(PATH_TO_CSV_FILE)\n",
    "assert np.array_equal(df2.to_numpy().flatten(),df.to_numpy().flatten())\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b944aaa1481e4ea8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_pca(data,k):\n",
    "    '''\n",
    "    data: a pandas data frame\n",
    "    Implement PCA on the dataset and return the transformed dataset as a pandas dataframe\n",
    "    keep k pricipal components\n",
    "    remember the dataset MUST have the chosen components and the columns you decide to exclude \n",
    "    from PCA calculation.\n",
    "    The returned dataframe should NOT have the index column\n",
    "    The returned dataframe should have similar column names as original and must be in the \n",
    "    SAME ORDER as in the original data, example : if X0 and X1 are chosen then X0 must appear\n",
    "    appear before X1 in the returned frame as it appears before in the original frame as well.\n",
    "    \n",
    "    Try to verify the function on your own :)\n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4db7e2b7485907b8",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_CSV_FILE = './data_pca.csv'\n",
    "assert np.array_equal(pd.read_csv('pca_4.csv').to_numpy(),apply_pca(data,4).to_numpy())\n",
    "\n",
    "#DO NOT ALTER THIS CELL\n",
    "#HIDDEN TEST CASES FOR apply_pca\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert np.array_equal(pd.read_csv('pca_5.csv').to_numpy(),apply_pca(data,5).to_numpy())\n",
    "assert np.array_equal(pd.read_csv('pca_7.csv').to_numpy(),apply_pca(data,7).to_numpy())\n",
    "assert np.array_equal(pd.read_csv('pca_10.csv').to_numpy(),apply_pca(data,10).to_numpy())\n",
    "### END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
