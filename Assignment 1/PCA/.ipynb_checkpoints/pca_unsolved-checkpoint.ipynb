{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 \n",
    "# Neural Networks and Fuzzy Logic (2nd Semester 2019-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "This assignment is designed to check your understanding of linear algebra, assuming you have basic knowlegde of Linear Algebra, this assignment will introudce you to some high level tasks in Linear Algebra, and what's the fun part about it? You'll be implemnting those concepts and visualizing them!! So Let's start.\n",
    "\n",
    "#### Key Concepts: Matrix Inverse, Linear Transforms, Eigendecomposition and all the interesting stuff Linear Algebra has :)\n",
    "\n",
    "##### Please make sure you adhere to the policy of originality of your work, the assignment should reflect your understanding and should be your original, cases of plagiarism will be strictly dealt with as per institute norms. The penalties can include debarrment from the course and registration for the next semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Matrices as linear transformations\n",
    "\n",
    "Matrices can be viewed as linear transformations you 'apply' to vectors. Some matrices will rotate your space, others will rescale it etc. So when we apply a matrix to a vector, we end up with a transformed version of the vector. When we say that we 'apply' the matrix to the vector it means that we calculate the dot product of the matrix with the vector. We will start with a basic example of this kind of transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No other imports are allowed !!!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function, please don't alter this\n",
    "def plot_vectors(vectors, cols, alpha=1):\n",
    "    \"\"\"\n",
    "    Plot set of vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vectors : array-like\n",
    "        Coordinates of the vectors to plot. Each vectors is in an array. For\n",
    "        instance: [[1, 3], [2, 2]] can be used to plot 2 vectors.\n",
    "    cols : array-like\n",
    "        Colors of the vectors. For instance: ['green', 'red'] will display the\n",
    "        first vector in green and the second in red.\n",
    "    alpha : float\n",
    "        Opacity of vectors\n",
    "    Returns:\n",
    "    fig : instance of matplotlib.figure.Figure\n",
    "        The figure of the vectors\n",
    "    \"\"\"\n",
    "    plt.axvline(x=0, color='#A9A9A9', zorder=0)\n",
    "    plt.axhline(y=0, color='#A9A9A9', zorder=0)\n",
    "\n",
    "    for i in range(len(vectors)):\n",
    "        if (isinstance(alpha, list)):\n",
    "            alpha_i = alpha[i]\n",
    "        else:\n",
    "            alpha_i = alpha\n",
    "        x = np.concatenate([[0,0],vectors[i]])\n",
    "        plt.quiver([x[0]],\n",
    "                   [x[1]],\n",
    "                   [x[2]],\n",
    "                   [x[3]],\n",
    "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
    "                  alpha=alpha_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 3], [2, 6]])\n",
    "print(A)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([[5], [3]])\n",
    "print(v)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the vector v with color red\n",
    "plot_vectors([v.flatten()],['red'])\n",
    "plt.ylim(0, 6)\n",
    "plt.xlim(0, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question**: did you see that v is flattened before plotting? why do we need to do that? Check out the code for plot_vectors and try to answer this? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write code for to apply a matrix A to a vector v, return the new vector\n",
    "# remember to replace the default return value\n",
    "\n",
    "def apply_linear_transform(v,A):\n",
    "    '''\n",
    "    Write your code here\n",
    "    \n",
    "    '''\n",
    "    return np.array([[4],[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for plotting the new vector and the old vector\n",
    "transformed_vector = apply_linear_transform(v,A)\n",
    "plot_vectors([v.flatten(), transformed_vector.flatten()],cols=['red','green'])\n",
    "plt.ylim(0, 6)\n",
    "plt.xlim(0, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "We have seen an example of a vector transformed by a matrix. Now imagine that the transformation of the initial vector gives us a new vector that has the exact same direction. The scale can be different but the direction is the same. Applying the matrix didn't change the direction of the vector. This special vector is called an eigenvector of the matrix. We will see that finding the eigenvectors of a matrix can be very useful.\n",
    "\n",
    "<span class='pquote'>\n",
    "    Imagine that the transformation of the initial vector by the matrix gives a new vector with the exact same direction. This vector is called an eigenvector of ${A}$.\n",
    "</span>\n",
    "\n",
    "This means that ${v}$ is a eigenvector of ${A}$ if ${v}$ and ${Av}$ are in the same direction or to rephrase it if the vectors ${Av}$ and ${v}$ are parallel. The output vector is just a scaled version of the input vector. This scalling factor is $\\lambda$ which is called the **eigenvalue** of ${A}$.\n",
    "\n",
    "$$\n",
    "{Av} = \\lambda{v}\n",
    "$$\n",
    "\n",
    "Talking in terms of eigen values and eigen vector the eigen decomposition of a matrix looks like the following:\n",
    "\n",
    "$$\n",
    "{A}= {V}\\cdot{diag}({\\lambda})\\cdot{V}^{-1}\n",
    "$$\n",
    "\n",
    "We will also verify this property of eigendecomposition.\n",
    "\n",
    "Eigen decomposition is a crucial step for PCA (introduced later)\n",
    "\n",
    "Let's begin!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_eigen_decomposition(matrix):\n",
    "    '''\n",
    "    use numpy to find the eigen decomposition of the matrix\n",
    "    returns eigenValues and the eigenVectors\n",
    "    '''\n",
    "    eigenValues = None\n",
    "    eigenVectors = None\n",
    "    return eigenValues, eigenVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify_eigen_decomposition(matrix,eigen_values,eigen_vectors):\n",
    "    \n",
    "    '''\n",
    "    verifies the eigen decomposition property of matrix\n",
    "    return true if values match else returns false\n",
    "    '''\n",
    "    reconstructed_matrix = None\n",
    "    return reconstructed_matrix == matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use some simulated data to find the eigendecomposition and verify the property\n",
    "\n",
    "matrix = np.array([[5., 1.],[3., 3.]])\n",
    "eigen_values, eigen_vectors = find_eigen_decomposition(matrix)\n",
    "result = verify_eigen_decomposition(matrix,eigen_values, eigen_vectors)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Dimensions is very important in Machine Leanring. The dimensions are all the features of the dataset. For instance, if you are looking at a dataset containing pieces of music, dimensions could be the genre, the length of the piece, the number of instruments, the presence of a singer etc. You can imagine all these dimensions as different columns. When there is only two dimensions, it is very convenient to plot: you can use the $x$ - and  $y$ -axis. Add color and you can represent a third dimension. It is similar if you have tens or hundereds of dimensions, it will just be harder to visualize it.\n",
    "\n",
    "When you have that many dimensions it happens that some of them are correlated. For instance, we can reasonably think that the genre dimension will correlate with the instruments dimensions in our previous example. One way to reduce dimensionality is simply to keep only some of them. The problem is that you loose good information. It would be nice to have a way to reduce these dimensions while keeping all the information present in the data set.\n",
    "\n",
    "The aim of principal components analysis (PCA) is generaly to reduce the number of dimensions of a dataset where dimensions are not completly decorelated. PCA provides us with a new set of dimensions, the principal components (PC).\n",
    "\n",
    "Step-wise PCA is:\n",
    "\n",
    "1) Center the data <br>\n",
    "2) Find the covariance matrix <br>\n",
    "3) Find the Eigen Decompostion <br>\n",
    "4) Choose the principal components <br>\n",
    "5) Find the new dataset Matrix <br>\n",
    "\n",
    "Following [article](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c) also explains PCA very well.\n",
    "Let's implement PCA to get an essence of how dimensions can be reduced, you need to implement the function one by one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is a 2D dataset which will help you visualize PCA\n",
    "#you will implement PCA on other dataset\n",
    "#you can use this sample data to visualize if your PCA works fine or not\n",
    "\n",
    "x = 5*np.random.rand(100)\n",
    "y = 2*x + 1 + np.random.randn(100)\n",
    "\n",
    "#the two dimensions of our dataset\n",
    "x = x.reshape(100, 1)\n",
    "y = y.reshape(100, 1)\n",
    "\n",
    "#The dataset matrix\n",
    "X = np.hstack([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot the dataset and see how it looks like\n",
    "plt.plot(X[:,0],X[:,1],'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center(X):\n",
    "    '''\n",
    "    function to center the data\n",
    "    modify the return value to return the modified matrix\n",
    "    '''\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_covariance_matrix(X):\n",
    "    '''\n",
    "    Find the covariance matrix of X\n",
    "    Return the final matrix\n",
    "    '''\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the centered data and the eigen vectors in the same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = center(X)\n",
    "covariance = find_covariance_matrix(X_centered)\n",
    "eigenVals, eigenVecs = find_eigen_decomposition(covariance)\n",
    "\n",
    "\n",
    "plot_vectors(eigenVecs.T, ['red', 'blue'])\n",
    "plt.plot(X_centered[:,0], X_centered[:,1], '*')\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_pca():\n",
    "    '''\n",
    "    using X_centered, eigenValues, eigenVecs as obtained above\n",
    "    apply pca on X_centered and chose the principal component\n",
    "    return the new matrix\n",
    "    '''\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize the transformed data\n",
    "data_transformed = visualize_pca()\n",
    "plt.plot(np.linspace(0,10,10),data_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Questions:** <br> \n",
    "1) Can you identify the color of the line which points to the direction of maximum variance? <br>\n",
    "2) Look at the Graph below and explain if PCA can be applied on this dataset or not? Do give reason. <br>\n",
    "3) Can you apply PCA to categorical variables? (Answer to this will give you hints for the next excercise)\n",
    "\n",
    "![Plot](images/pca_inline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on a large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset:\n",
    "Task: Reducing the dimesions for predicting house prices.\n",
    "The dataset is stored in the file **'data_pca.csv'**, it has 15 numerical columns and 2 categorical columns (columns are labeled as $X0$....$X16$), in an imaginary world, all these columns have certain significance towards prediciton of final house prices. Your task is simple, apply PCA to reduce the number of dimensions. \n",
    "\n",
    "Now implement the following functions for getting the final dataset with reduced dimensions. The number of principal components to keep is your choice :) but make sure you keep the maximum information while simultaneously eliminating the redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(name):\n",
    "    '''\n",
    "    read the CSV file and return the corresponding numpy array\n",
    "    '''\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_pca(data):\n",
    "    '''\n",
    "    Implement PCA on the dataset and return the transformed dataset\n",
    "    '''\n",
    "    transformed_data = None\n",
    "    return transformed_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
